{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단어 임베딩 튜토리얼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('/notebooks/embedding'))\n",
    "sys.path.insert(0, os.path.abspath('/notebooks/embedding/models'))\n",
    "\n",
    "os.chdir('/notebooks/embedding')\n",
    "#sys.path.append('/notebooks/embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget https://github.com/dongjun-Lee/kor2vec/raw/master/test_dataset/kor_ws353.csv -P /notebooks/embedding/data/raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.word_eval import WordEmbeddingEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 코드 4-5 Word2Vec 코사인 유사도 상위 단어 목록 체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs_txt_fname='/notebooks/embedding/data/word-embeddings/word2vec/word2vec'\n",
    "method='word2vec'\n",
    "dim=100\n",
    "tokenizer_name='mecab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.word_eval import WordEmbeddingEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WordEmbeddingEvaluator(vecs_txt_fname=vecs_txt_fname, \n",
    "                               method=method, \n",
    "                               dim=dim, \n",
    "                               tokenizer_name=tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('소망', 0.79329586),\n",
       " ('행복', 0.7861444),\n",
       " ('희망찬', 0.76918393),\n",
       " ('꿈', 0.76410115),\n",
       " ('열망', 0.7336163)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('희망', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._is_in_vocabulary('서울특벌시')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('특별시', 0.7592423437826079),\n",
       " ('동대문', 0.7466565542196317),\n",
       " ('관악', 0.7372439826844083),\n",
       " ('영등포', 0.7357625674365258),\n",
       " ('서대문', 0.7306555874308116)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('서울특벌시', topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 코드 4-9 FastText Skip-gram 모델의 코사인 유사도 상위 단어 목록 체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs_txt_fname='/notebooks/embedding/data/word-embeddings/fasttext/fasttext.vec'\n",
    "vecs_bin_fname='/notebooks/embedding/data/word-embeddings/fasttext/fasttext.bin'\n",
    "method='fasttext'\n",
    "dim=100\n",
    "tokenizer_name='mecab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.word_eval import WordEmbeddingEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = WordEmbeddingEvaluator(vecs_txt_fname=vecs_txt_fname, \n",
    "                               vecs_bin_fname=vecs_bin_fname, \n",
    "                               method=method, \n",
    "                               dim=dim, \n",
    "                               tokenizer_name=tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('행복', 0.779841546336235),\n",
       " ('희망찬', 0.7223989696190132),\n",
       " ('소망', 0.7158186282535396),\n",
       " ('땀방울', 0.6873366793848128),\n",
       " ('희망특강', 0.6866479743108849)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('희망', topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그림 4-12 '하였다'와 가장 유사한 FastText 단어 목록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('하', 0.9295729665862918),\n",
       " ('다', 0.907324941314357),\n",
       " ('했', 0.8929994169029608),\n",
       " ('였으며', 0.8632510577839813),\n",
       " ('했으며', 0.8549427906656639)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('하였다', topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그림 4-13 미등록 단어에 대한 FastText 임베딩 체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._is_in_vocabulary('서울특벌시')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.34943753,  0.35756463, -0.13093197, -0.2630131 , -0.46309552,\n",
       "       -0.06662391, -0.01265321, -0.11453624,  0.20499213,  0.25899005],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_word_vector('서울특벌시')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('서울색', 0.7196167662285975),\n",
       " ('서울한강체', 0.661677125632246),\n",
       " ('서울새남굿', 0.6590039219164663),\n",
       " ('철화문', 0.65209296055566),\n",
       " ('서울서체', 0.6515671876001969)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('서울특벌시', topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 코드 4-12 한글 자소분해 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ㄴㅏ- ㄴㅡㄴ ㅎㅏㄱㄱㅛ- ㅇㅔ- ㄱㅏㄴㄷㅏ-\n"
     ]
    }
   ],
   "source": [
    "from preprocess import jamo_sentence, get_tokenizer\n",
    "tokenizer = get_tokenizer(\"mecab\")\n",
    "tokens = \" \".join(tokenizer.morphs(\"나는 학교에 간다\"))\n",
    "print(jamo_sentence(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 코드 4-13 은전한닢  mecab으로 형태소 분석된 말뭉치를 자소 단위로 분해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"preprocess/unsupervised_nlputils.py\", line 170, in <module>\r\n",
      "    process_jamo(args.input_path, args.output_path)\r\n",
      "  File \"preprocess/unsupervised_nlputils.py\", line 141, in process_jamo\r\n",
      "    processed_sentence = jamo_sentence(sentence)\r\n",
      "  File \"preprocess/unsupervised_nlputils.py\", line 128, in jamo_sentence\r\n",
      "    if character_is_korean(char):\r\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/soynlp/hangle/_hangle.py\", line 94, in character_is_korean\r\n",
      "    i = to_base(c)\r\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/soynlp/hangle/_hangle.py\", line 106, in to_base\r\n",
      "    def to_base(c):\r\n",
      "KeyboardInterrupt\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/notebooks/embedding')\n",
    "! python preprocess/unsupervised_nlputils.py --preprocess_mode jamo \\\n",
    "            --input_path /notebooks/embedding/data/tokenized/corpus_mecab.txt \\\n",
    "            --output_path /notebooks/embedding/data/tokenized/corpus_mecab_jamo.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/notebooks/embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p data/word-embeddings/fasttext-jamo\n",
    "! models/fastText/fasttext skipgram \\\n",
    "  -input data/tokenized/corpus_mecab_jamo.txt \\\n",
    "  -output data/word-embeddings/fasttext-jamo/fasttext-jamo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 코드 4-15 자소 단위 FastText Skip-gram 모델의 유사어 상위 목록 체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs_txt_fname='/notebooks/embedding/data/word-embeddings/fasttext-jamo/fasttext-jamo.vec'\n",
    "vecs_bin_fname='/notebooks/embedding/data/word-embeddings/fasttext-jamo/fasttext-jamo.bin'\n",
    "method='fasttext-jamo'\n",
    "dim=100\n",
    "tokenizer_name='mecab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from models.word_eval import WordEmbeddingEvaluator\n",
    "model = WordEmbeddingEvaluator(vecs_txt_fname=vecs_txt_fname, \n",
    "                               vecs_bin_fname=vecs_bin_fname, \n",
    "                               method=method, \n",
    "                               dim=dim, \n",
    "                               tokenizer_name=tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('희망찬', 0.8146420631099898),\n",
       " ('행복', 0.782225588355358),\n",
       " ('희망특강', 0.7528447282282469),\n",
       " ('희망자', 0.7333699883345198),\n",
       " ('소망', 0.7317962077164233)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('희망', topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 코드 4-16 미등록 단어에 대한 자소 단위 FastText 임베딩 체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._is_in_vocabulary(\"서울특벌시\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.27308005, -0.03842273, -0.14564085, -0.6462154 ,  0.11989901,\n",
       "        0.3353665 ,  0.03407207, -0.02902705,  0.38201326, -0.22358143],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_word_vector(\"서울특벌시\")[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('서울시', 0.7747652879791547),\n",
       " ('특별시', 0.767032326418248),\n",
       " ('서울특별시장', 0.7537271226369762),\n",
       " ('특별시세', 0.736772918595682),\n",
       " ('성동격서', 0.736006622807833)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('서울특벌시', topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA 잠재 의미 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 코드 4-22  코사인 유사도 상위 단어 목록 체크 (단어-문맥 행렬 + LSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs_txt_fname='/notebooks/embedding/data/word-embeddings/lsa/lsa-cooc.vecs'\n",
    "method='lsa'\n",
    "dim=100\n",
    "tokenizer_name='mecab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.word_eval import WordEmbeddingEvaluator\n",
    "model = WordEmbeddingEvaluator(vecs_txt_fname=vecs_txt_fname,\n",
    "                               method=method, \n",
    "                               dim=dim, \n",
    "                               tokenizer_name=tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('진실', 0.9481062178606177),\n",
       " ('의식', 0.9450190576762693),\n",
       " ('즐거움', 0.9365101544335024),\n",
       " ('사냥', 0.933343812149678),\n",
       " ('인내심', 0.9328922247781231)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('희망', topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 코드 4-23 코사인 유사도 상위 단어 목록 체크 (PPMI + LSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs_txt_fname='/notebooks/embedding/data/word-embeddings/lsa/lsa-pmi.vecs'\n",
    "method='lsa'\n",
    "dim=100\n",
    "tokenizer_name='mecab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.word_eval import WordEmbeddingEvaluator\n",
    "model = WordEmbeddingEvaluator(vecs_txt_fname=vecs_txt_fname,\n",
    "                               method=method, \n",
    "                               dim=dim, \n",
    "                               tokenizer_name=tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('진실', 0.9483922027456372),\n",
       " ('의식', 0.9451445814676003),\n",
       " ('즐거움', 0.9365892430718368),\n",
       " ('사냥', 0.9333096097760885),\n",
       " ('인내심', 0.9329586789540774)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('희망', topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 코드 4-27 GloVe 모델의 코사인 유사도 상위 단어 체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs_txt_fname='/notebooks/embedding/data/word-embeddings/glove/glove.txt'\n",
    "method='glove'\n",
    "dim=100\n",
    "tokenizer_name='mecab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WordEmbeddingEvaluator(vecs_txt_fname=vecs_txt_fname, \n",
    "                               method=method, \n",
    "                               dim=dim, \n",
    "                               tokenizer_name=tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('행복', 0.7593905742455305),\n",
       " ('꿈', 0.7190308221412974),\n",
       " ('사랑', 0.6961724218705125),\n",
       " ('미래', 0.6795513141617532),\n",
       " ('세상', 0.672559694141116)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('희망', topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swivel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 코드 4-31 Swivel 모델의 코사인 유사도 목록 체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs_txt_fname='/notebooks/embedding/data/word-embeddings/swivel/row_embedding.tsv'\n",
    "method='swivel'\n",
    "dim=100\n",
    "tokenizer_name='mecab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WordEmbeddingEvaluator(vecs_txt_fname=vecs_txt_fname, \n",
    "                               method=method, \n",
    "                               dim=dim, \n",
    "                               tokenizer_name=tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('행복', 0.6927502964154575),\n",
       " ('꿈', 0.6193135728601431),\n",
       " ('우리', 0.5831879804568582),\n",
       " ('젊은이', 0.5795773384886462),\n",
       " ('사랑', 0.5489307751831267)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('희망', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec: spearman corr: 0.5770993871014621 , pearson corr: 0.5956751142850295 \n",
    "#  fasttext: spearman corr: 0.5770993871014621 , pearson corr: 0.5956751142850295\n",
    "# glove: spearman corr: 0.49029953452220065 , pearson corr: 0.5383746018370396\n",
    "# swivel: spearman corr: 0.549541215508716 , pearson corr: 0.5727286333920304"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
