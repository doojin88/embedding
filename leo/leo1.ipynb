{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname('.'), '/notebooks/embedding')))\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname('.'), '/notebooks/embedding/models')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, os, sys, math\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from soynlp.word import pmi\n",
    "from soynlp.vectorizer import sent_to_word_contexts_matrix\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "from preprocess import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/embedding/leo\r\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태소분석 완료된 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download tokenized data...\n",
      "--2020-05-10 06:28:42--  https://docs.google.com/uc?export=download&confirm=Htbi&id=1QEdjvT0Jpqmz9F57ATmjy3i016kdcURq\n",
      "Resolving docs.google.com (docs.google.com)... 172.217.25.110, 2404:6800:4004:810::200e\n",
      "접속 docs.google.com (docs.google.com)|172.217.25.110|:443... 접속됨.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-08-48-docs.googleusercontent.com/docs/securesc/7s2uhg53urb8klu1tj71q3vvubg9fprp/7s9oon2m7ki3at8nr44nd2blu7oq7ldd/1589092050000/05170634686643261154/08224146599082605233Z/1QEdjvT0Jpqmz9F57ATmjy3i016kdcURq?e=download [following]\n",
      "--2020-05-10 06:28:42--  https://doc-08-48-docs.googleusercontent.com/docs/securesc/7s2uhg53urb8klu1tj71q3vvubg9fprp/7s9oon2m7ki3at8nr44nd2blu7oq7ldd/1589092050000/05170634686643261154/08224146599082605233Z/1QEdjvT0Jpqmz9F57ATmjy3i016kdcURq?e=download\n",
      "Resolving doc-08-48-docs.googleusercontent.com (doc-08-48-docs.googleusercontent.com)... 172.217.175.65, 2404:6800:4004:80a::2001\n",
      "접속 doc-08-48-docs.googleusercontent.com (doc-08-48-docs.googleusercontent.com)|172.217.175.65|:443... 접속됨.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://docs.google.com/nonceSigner?nonce=hrt69s48di6tg&continue=https://doc-08-48-docs.googleusercontent.com/docs/securesc/7s2uhg53urb8klu1tj71q3vvubg9fprp/7s9oon2m7ki3at8nr44nd2blu7oq7ldd/1589092050000/05170634686643261154/08224146599082605233Z/1QEdjvT0Jpqmz9F57ATmjy3i016kdcURq?e%3Ddownload&hash=ehe8r91l8bttgivhak66oardvmtuindt [following]\n",
      "--2020-05-10 06:28:43--  https://docs.google.com/nonceSigner?nonce=hrt69s48di6tg&continue=https://doc-08-48-docs.googleusercontent.com/docs/securesc/7s2uhg53urb8klu1tj71q3vvubg9fprp/7s9oon2m7ki3at8nr44nd2blu7oq7ldd/1589092050000/05170634686643261154/08224146599082605233Z/1QEdjvT0Jpqmz9F57ATmjy3i016kdcURq?e%3Ddownload&hash=ehe8r91l8bttgivhak66oardvmtuindt\n",
      "접속 docs.google.com (docs.google.com)|172.217.25.110|:443... 접속됨.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://doc-08-48-docs.googleusercontent.com/docs/securesc/7s2uhg53urb8klu1tj71q3vvubg9fprp/7s9oon2m7ki3at8nr44nd2blu7oq7ldd/1589092050000/05170634686643261154/08224146599082605233Z/1QEdjvT0Jpqmz9F57ATmjy3i016kdcURq?e=download&nonce=hrt69s48di6tg&user=08224146599082605233Z&hash=c14khlhh1j8hmo5ik2fgttm2s1246tpi [following]\n",
      "--2020-05-10 06:28:43--  https://doc-08-48-docs.googleusercontent.com/docs/securesc/7s2uhg53urb8klu1tj71q3vvubg9fprp/7s9oon2m7ki3at8nr44nd2blu7oq7ldd/1589092050000/05170634686643261154/08224146599082605233Z/1QEdjvT0Jpqmz9F57ATmjy3i016kdcURq?e=download&nonce=hrt69s48di6tg&user=08224146599082605233Z&hash=c14khlhh1j8hmo5ik2fgttm2s1246tpi\n",
      "접속 doc-08-48-docs.googleusercontent.com (doc-08-48-docs.googleusercontent.com)|172.217.175.65|:443... 접속됨.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/zip]\n",
      "Saving to: ‘/notebooks/embedding/data/tokenized.zip’\n",
      "\n",
      "/notebooks/embeddin     [     <=>            ] 838.07M  15.8MB/s    in 55s     \n",
      "\n",
      "2020-05-10 06:29:38 (15.3 MB/s) - ‘/notebooks/embedding/data/tokenized.zip’ saved [878775400]\n",
      "\n",
      "Archive:  tokenized.zip\n",
      "   creating: tokenized/\n",
      "  inflating: tokenized/korquad_mecab.txt  \n",
      "  inflating: tokenized/wiki_ko_mecab.txt  \n",
      "  inflating: tokenized/corpus_mecab_jamo.txt  \n",
      "  inflating: tokenized/ratings_okt.txt  \n",
      "  inflating: tokenized/ratings_khaiii.txt  \n",
      "  inflating: tokenized/ratings_hannanum.txt  \n",
      "  inflating: tokenized/ratings_soynlp.txt  \n",
      "  inflating: tokenized/ratings_mecab.txt  \n",
      "  inflating: tokenized/ratings_sentpiece.txt  \n",
      "  inflating: tokenized/ratings_komoran.txt  \n"
     ]
    }
   ],
   "source": [
    "! cd /notebooks/embedding && bash preprocess.sh dump-tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge datasets..\r\n"
     ]
    }
   ],
   "source": [
    "! cd /notebooks/embedding && bash wordmodel.sh merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_save_path(full_path):\n",
    "    if full_path[:4] == \"data\":\n",
    "        full_path = os.path.join(os.path.abspath(\".\"), full_path)\n",
    "    model_path = '/'.join(full_path.split(\"/\")[:-1])\n",
    "    if not os.path.exists(model_path):\n",
    "       os.makedirs(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Skip-gram 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p /notebooks/leo/word-embedding/word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1eb45ae07772>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_fname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You can't pass a generator as the sentences argument. Try a sequence.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m             self.train(\n\u001b[1;32m    761\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, sentences, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    934\u001b[0m         \"\"\"\n\u001b[1;32m    935\u001b[0m         total_words, corpus_count = self.vocabulary.scan_vocab(\n\u001b[0;32m--> 936\u001b[0;31m             sentences=sentences, corpus_file=corpus_file, progress_per=progress_per, trim_rule=trim_rule)\n\u001b[0m\u001b[1;32m    937\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[0;34m(self, sentences, corpus_file, progress_per, workers, trim_rule)\u001b[0m\n\u001b[1;32m   1589\u001b[0m             \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLineSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1591\u001b[0;31m         \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scan_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m         logger.info(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m_scan_vocab\u001b[0;34m(self, sentences, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m   1573\u001b[0m                 )\n\u001b[1;32m   1574\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1575\u001b[0;31m                 \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1576\u001b[0m             \u001b[0mtotal_words\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "corpus_fname = \"/notebooks/embedding/data/tokenized/corpus_mecab.txt\"\n",
    "model_fname = \"/notebooks/embedding/leo/word-embedding/word2vec/word2vec\"\n",
    "    \n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "corpus = [sent.strip().split(\" \") for sent in open(corpus_fname, 'r').readlines()]\n",
    "\n",
    "model = Word2Vec(corpus, size=100, workers=4, sg=1)\n",
    "model.save(model_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python script 를 이용한 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```bash\n",
    "cd /notebooks/embedding\n",
    "mkdir -p /notebooks/embedding/data/word-embeddings/word2vec\n",
    "python models/word_utils.py --method train_word2vec \\\n",
    "\t--input_path /notebooks/embedding/data/tokenized/corpus_mecab.txt \\\n",
    "\t--output_path /notebooks/embedding/data/word-embeddings/word2vec/word2vec\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word-embeddings 모델 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo `date` && cd /notebooks/embedding && bash preprocess.sh dump-word-embedding && echo `date`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting:2020. 05. 10. (일) 08:37:45 UTC\r\n",
      "Ended:2020. 05. 10. (일) 08:37:45 UTC\r\n"
     ]
    }
   ],
   "source": [
    "! echo \"Starting:`date`\" && cd /notebooks/embedding && bash preprocess.sh dump-word-embedding && echo \"Ended:`date`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
